Hello, everyone! The title of my project is "Space Touching Technology in PC platform". The presentation will be divided in 3 parts. The first part is the origin source of my idea and its potential application, the second is the current technology concerning about this target, and the finally part is how we can  achieve our goal in a series of epics. OK let me start.

I come up with this idea when I am reading ebooks. Sometime I want to put my hands in a comfortable position, but I found it's impossible. I have to keep my hand near the laptop's keyboard because I should turn the page by my lovely fat hand. One time I think about the new technology released by Huawei - the Space Touching Technology. If I can just rise my hand at a distance of screen to turn the page, how it will be relaxing! So I have this idea since then. At the same time, I searched and know the more interesting way to interact with computer of this Space Touching Technology -- switch left and right on photos, videos, or ebooks; switch up and down on web page, documents, news or tik-tok; push forward your hand palms to give somebody a like.

So we can present its potential usage in a wide view: not only switch videos in a more comfortable way when we are addicted in Bilibili or YouTube, turn pages just by rising up your hand on Duokan Reader or Apple iBooks. Reading web pages like Toutiao or Zhihu just slide up and down our hand. More interesting application field is game. Can you image you can cut down the enemy in Sekiro or Assassin's Creed, and block enemy's attack by holding your hand horizon, like this. An other common thought is build a virtual controller in air to command your hero in LOL ,Chicken Eating or Hearthstone. We can put our sight in a further position, what will happen if we can precisely tracing the motivation of finger? Maybe we can control the play progress in air, typing in virtual keyboard or unlock your computer in a more funny way? E, maybe it is better for us to not let imagine running too forward that reality. I will present possible technology and solution for our project.

There are two major industrial technical branches based on the different sensor type. One use light sensor and the other use millimeter-wave radar. A typical example of the first branch is Huawei's solution. It use a common camera to get the color feature and a Tof(the abbreviation of Time of flight) laser depth camera to get depth information. Its limitation is because of reflect, the depth information measured by time of light flight  sometimes isn't correct. The other's typical project is Google Soli. It use millimeter-wave radar to detect tiny motion of finger to sense user's gesture. Its advantage is the the precise is extremely high, and its limitation is the technic is not proven and the method to deal with radar data is very different to picture format. These are possible solution in industrial, now I will present our road map to achieve our solutions.

The device I decided to use is just common camera that every laptop equipped in the project. The first reason is depth camera, millimeter-wave radar and the corresponding data is not easy to access. And the second reason is that deal with component data need more complicated technics which difficult for us. Just image data is more easy. 

We needn't to build the application just from the air. Google support a good basement for us named Media Pipe. It is a cross-form, customizable ML solution for live and streaming media, corresponded to the camera data we will use. The hand critical point recognition parts can position twenty-one critical point of our point, which make it easy to processing the raw camera data to the gesture signal. By the way, If we feel difficult when get down to the platform, we can chose a more simple way to achieve our goal. We can use common CNN(Convolutional Neural Network) to just recognize hand shape and then simply conduct command like virtually press "right arrow" key. Concerning lessons can be found in Kaggle.com, which in the section of Computer vision. That's not all, the project can be continuous developed. Here are my planned _ epics to take over the project.

The first epic is develop a simple hand recognizer that can tell if the camera see or not see a hand palm. If the hand exist in the sight of camera, some operation will be done. We can just use palm recognition technics that just need a several layer of neral network. It's easy and can be fast achieved. The second epic is adding some extra information to the palm, like position information, so we can tracing palm's motivation in initial stage. Because of the position feature, we can control the computer by difference about palm motivating. The third epic is not only tracing the hand, but also tracing 21 critical point of palm, and train our model to distinguish different finger gesture like pinch or spread to provide more concise of control. The final epic I can imagine is that use physical engine added to the model to map the gesture to reality world behavior, like roll a controller ball, turn a controller wheel, or others. It's a challenging and difficult task. Alternatively we can switch to the radar technic branch. Radar can detect tiny motion that plain camera can't do.

As a conclusion, this project is easy to settle down to start, and have very wide potential usage, existing similar solution can be referred to. and with incremental develop epics. For this project can not just be a project on paper, I need all of your help. Thank you for listening. Arthurfish